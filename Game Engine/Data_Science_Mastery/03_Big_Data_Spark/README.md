# Big Data with Apache Spark

Handling data that is too big for a single computer.

## Concepts
- **Distributed Computing**: Breaking a large task into smaller ones across many machines.
- **RDDs and DataFrames**: Spark's distributed data structures.
- **Lazy Evaluation**: Spark doesn't do anything until you ask for a result.
- **Spark SQL**: Using SQL to query large datasets.

## Challenge
1. Research what a "Cluster" is in the context of Big Data.
2. Read about the "MapReduce" paradigm that Spark is based on.
3. Compare the performance of Spark vs. traditional Pandas for a 100GB dataset (Research).
